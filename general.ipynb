{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-NEAREST NEIGHBORS (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is a superwised learning algorithm used for both classification and regression tasks. It is a non-parametric, instance-based (lazy) learning algorithm that makes predictions based on the similarity between data points.\n",
    "\n",
    "* In ***kNN classification***, the output is a class membership. The given data point is classified based on the majority of type of its neighbours. The data point is assigned to the most frequent class among its *k* nearest neighbours. Usually *k* is a small positive integer. If *k*=1, then the data point is simply assigned to the class of that single nearest neighbour.\n",
    "\n",
    "* In ***kNN regression***, the output is simply some property value for the object. This value is the average of the values of *k* nearest neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADVANTAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***simple and easy to understand***: intuative and straightforward algorithm that doesn't require complex model training\n",
    "* ***no training phase (lazy learning)***: doesn't require model training, instead stores all data points and makes predictions at runtime\n",
    "* **works well with small datasets***: performes well when there are fewer data points and fewer features\n",
    "* ***can be used for classification and both regression***: it is versatile and works for both classification and regression tasks\n",
    "* ***non-parametric***: does not assume any specific data distribution, making useful for a variety of real-world problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISADVANTAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***slow with large datasets***: since KNN calculates the distance between the query point and all data points, it becomes very slow for large datasets\n",
    "* ***memory-intensive***: needs to store the entire dataset, which comsumes a lot of memory\n",
    "* ***sensitive to irrelevant features and noise***: if there are many irrelevant features or noisy data points, KNN's performance decreases. Feature selection or normalization is often needed\n",
    "* ***choosing the right 'K' is tricky***: selecting the best value for K is crusial (too small K - overfitting, too large k is underfitting)\n",
    "* ***not good for high-dimentional data***: as the number of features increases, distance cealculations become less meaningful due to the curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THEORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBRARIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ***scikit-learn***: a general-purpose KNN that is easy to use, well-optimized, and widely adopted in both industry and academia.\n",
    "    * `KNeighborsClassifier` - for classification\n",
    "    * `KNeighborsRegressor` - for regression\n",
    "    * `KNNImputer` - for handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ***faiss***: a large-scale KNN that is extremely fast for high-dimensional large datasets (millions of points) and optimized by Facebook AI.\n",
    "\n",
    "    * faiss.IndexFlatL2 - for fast nearest-neighbor search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install faiss-cpu (for CPU version)\n",
    "# pip install faiss-gpu (for GPU version)\n",
    "\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ***n_neighbors (K)*** (Number of Neighbors): specifies the number of closest data points to consider for classification or regression. Selecting an appropriate value for *K* is essentialâ€”if *K* is too small, the model may have high variance, resulting in *overfitting*, while a very large *K* can lead to high bias, causing *underfitting*.\n",
    "\n",
    "    BEST PRACTICE: use cross-validation (GridSearchCV) to find the optimal *K*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ***metric*** (Distance Metric): specifies how the algorithm measures the distance between points. Options:\n",
    "\n",
    "    * *Euclidean Distance* - best for continuous data (default)\n",
    "    * *Manhattan Distance* - better for grid-like data, e.g., city blocks\n",
    "    * *Minkowski Distance* - generalized form\n",
    "    * *Hamming Distane* - for categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ***weights*** - Weighting of Neighbors: This parameter specifies how much influence each neighbor has. Options:\n",
    "    * *iniform* - all neighbours contribute equally\n",
    "    * *distance* - closer neighbours have more influence\n",
    "\n",
    "    BEST PRACTICE: *'distance'* is better for datasets where closer points are more relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ***algorithm*** - Search Algorithm for Nearest Neighbors: This parameter specifies how neighbors are serached. Options:\n",
    "    * *auto* - automatically selects the best method (default)\n",
    "    * *ball_tree* - good for meium-sized data\n",
    "    * *kd_tree* - efficient for low-dimensional data\n",
    "    * *brute* - slower but works for all cases\n",
    "\n",
    "    BEST PRACTICE: leave it as *'auto'*, unless you have a large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': range(1, 20),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'hamming']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best Parameters:', grid.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
